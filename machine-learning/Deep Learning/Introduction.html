<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-10-15 Sun 11:30 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta name="generator" content="Org mode" />
<meta name="author" content="nerdsville" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6657e0a">1. Deep Learning Book</a>
<ul>
<li><a href="#orgd50bbb7">1.1. Introduction</a></li>
<li><a href="#org6087c13">1.2. Knowledge Base AI</a></li>
<li><a href="#org4ccd772">1.3. AI Deep Learning</a>
<ul>
<li><a href="#org238af21">1.3.1. History</a></li>
<li><a href="#org605d08c">1.3.2. Cybernetics/Linear Models</a></li>
</ul>
</li>
<li><a href="#org70aa500">1.4. Machine Learning (ML)</a></li>
<li><a href="#orge609682">1.5. Representation Learning</a></li>
<li><a href="#org9706d24">1.6. Design of Learning Features</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org6657e0a" class="outline-2">
<h2 id="org6657e0a"><span class="section-number-2">1</span> Deep Learning Book</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-orgd50bbb7" class="outline-3">
<h3 id="orgd50bbb7"><span class="section-number-3">1.1</span> Introduction</h3>
<div class="outline-text-3" id="text-1-1">
<p>
In the early days of AI, the field rapidly solved problems which were
intellectually difficult for humans
</p>
<ul class="org-ul">
<li>These problems were able to be broken down into
sequential mathematical rules.</li>
</ul>
<p>
The challenge to AI has been solving tasks which are easy for people to perform but
difficult for people to describe.
These problems consist of:
</p>
<ul class="org-ul">
<li>Recognizing people</li>
<li>Recognizing spoken words</li>
</ul>
<p>
Early versions of AI took place in formal environments and did not require computers
to know much about the world.
</p>
<ul class="org-ul">
<li>IBM's Deep Blue chess-playing system defeated
the world champion of the time.</li>
<li>Chess is a very simple world
<ul class="org-ul">
<li>Contains only 64 locations</li>
<li>Has only 32 pieces that can move in very
rigidly defined ways</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org6087c13" class="outline-3">
<h3 id="org6087c13"><span class="section-number-3">1.2</span> Knowledge Base AI</h3>
<div class="outline-text-3" id="text-1-2">
<p>
One of the challenges in AI is getting the informal knowledge into a computer
There have been many AI projects which hard-coded knowledge about the world in formal language
Cyc is one of the most famous projects using this technique
</p>
<ul class="org-ul">
<li>It is an "inference engine" and database of statements in a language called CycL.</li>
<li>Statements manually inputted by humans.</li>
<li>Incredibly difficult for humans to come up with
formal rules with enough complexity to actually describe the world</li>
<li>Cyc failed to understand a story about a person named Fred
shaving in the morning
<ul class="org-ul">
<li>Root of this misunderstanding was with associating a human w/
electrical parts.</li>
<li>Associated this example with a completely different entity.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org4ccd772" class="outline-3">
<h3 id="org4ccd772"><span class="section-number-3">1.3</span> AI Deep Learning</h3>
<div class="outline-text-3" id="text-1-3">
<p>
This solution is allowing computers to:
</p>
<ul class="org-ul">
<li>Learn from experience
<ul class="org-ul">
<li>Enables the avoidance of the need for human operators
to formally specify knowledge.</li>
</ul></li>
<li>Understand the world in terms of a hierarchy
of concepts
<ul class="org-ul">
<li>Each concept is defined through its relation
to simpler concepts</li>
<li>Enables computers to learn complicated concepts</li>
</ul></li>
</ul>
<p>
Drawing a graph to represent these concepts relationships produces a graph
that is very deep, hence the name <i>Deep Learning</i>
</p>

<p>
A good example of <i>Deep Learning</i> is the <i>Multilayer Perceptron</i> (MLP)
An MLP is just a mathematical function which the application of provides a new
<i>representation</i> of the input.
</p>

<p>
The depth of deep learning enables the computer to learn a multistep computer program
and provides a sort of "stack" which each level can refer to the previous levels, increasing
the level of detail.
</p>

<p>
Deep learning breaks a complicated mapping into a series of simple nested mappings
</p>
<dl class="org-dl">
<dt>Visible Layer</dt><dd>The variables we are able to observe</dd>
<dt>Hidden Layer</dt><dd>Values not given in the data, data which the model deems as useful for
explaining the relationships in the observed data</dd>
</dl>

<p>
There are two main ways of measuring the depth of a model:
</p>
<ul class="org-ul">
<li>The number of sequential instructions that must be executed to evaluate the architecture</li>
<li>The depth of the graph of the concepts relations to each other (used by <i>deep probabilistic models</i>)
<ul class="org-ul">
<li>This is usually much shorter than the number of computational instructions</li>
</ul></li>
</ul>
<p>
There is no single correct value for the depth of an architecture, just as there is no single correct value
for the length of a computer program
</p>

<p>
Deep learning has become more useful as the amount of avaialable training data has increased.
</p>
</div>

<div id="outline-container-org238af21" class="outline-4">
<h4 id="org238af21"><span class="section-number-4">1.3.1</span> History</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
Deep learning has a long history, dating back to the 1940's.
</p>

<p>
There have been three waves of development:
</p>
<ul class="org-ul">
<li>Between the 1940s - 1960s it was known as <i>cybernetics</i></li>
<li>Between the 1980s - 1990s it was known as <i>connectionism</i></li>
<li>The current naming began in 2006</li>
</ul>

<p>
Another name that Deep Learning has gone by is <i>artificial neural networks</i> (ANNs)
</p>
<ul class="org-ul">
<li>The idea behind this is that deep learning models are engineered systems inspired by the
biological brain</li>
</ul>
<p>
The neural perspective on deep learning is motivated by two ideas
</p>
<ul class="org-ul">
<li>The brain provides a proof of example that intelligent behavior is possible, and a straightforward
path to building AI is to reverse engineer it.</li>
<li>It would be interesting to understand the brain and the principles that underlie human intelligence.</li>
</ul>
<p>
The modern term goes beyond neuroscientific perspectives and appeals to a principle of learning <i>multiple
levels of composition</i>, which can be applied to non-neurally inspired ML frameworks.
The earliest predecessors of modern deep learning were simple linear models motivated from a neuroscientific
perspective.
 The <i>neocognitron</i> (1980) introduced a powerful model architecture for processing images, and was inspired by the
structure of the mammalian visual system, this later becomes the basis of the modern <i>convolutional network</i> (1988)
</p>
</div>
</div>
<div id="outline-container-org605d08c" class="outline-4">
<h4 id="org605d08c"><span class="section-number-4">1.3.2</span> Cybernetics/Linear Models</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
These were designed to take a set of <i>n</i> input values x<sub>1</sub>,&#x2026;,x<sub>n</sub> and associate them with an output <i>y</i>
and would learn a set of weights w<sub>1</sub>,&#x2026;,w<sub>n</sub> and compute their output.
</p>
<ul class="org-ul">
<li>f(x, w) = x<sub>1</sub>w<sub>1</sub> + &#x2026; + x<sub>n</sub>w<sub>n</sub></li>
</ul>
<p>
The <i>perceptron</i>, from the 1950's, was the earliest model which could learn the weights that defined the categories when given examples of inputs
from each category
</p>

<p>
The <i>adaptive linear element</i> (ADALINE), which dates to around the same time, simply returned the value of
f(x) itself to predict a real number, and can be used to predict these numbers from data.
</p>

<p>
The training algorithm called <i>stochastic gradient descent</i> could use these weights and adapt them. Slightly
modified versions of this algorithm are the dominant algorithms used to this day.
</p>

<p>
Linear models have many limitations. Most famously they can not learn the XOR function, where:
  f([0,1], w) = 1, and f([1,0], w) = 1, but f([1,1], w) = 0 and f([0,0], w) = 0
This is one of the reasons why neuroscience is no longer the predominant guide for deep learning, it is, however,
still an important source of inspiration for deep learning researchers.
We simply do not have enough information about the brain to use it as a guide.
</p>

<p>
The basic idea of having many compulational units which become intelligent only via their interactions is
inspired by the brain.
</p>

<p>
The endeavor to understand how the brain works on an algorithmic level is known as <i>computational neuroscience</i>
The field of deep learning is primarily concerned with how to build computer systems which are able to
solve tasks requiring intelligence, while computational neruroscience is concerned with building more accurate
models of how the brain actually works.
</p>
</div>
</div>
</div>


<div id="outline-container-org70aa500" class="outline-3">
<h3 id="org70aa500"><span class="section-number-3">1.4</span> Machine Learning (ML)</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Enables automated tackling of problems involving knowledge of the real world to
intelligently create solutions which appear subjective
The performance of these simple ML algorithms depends on the <i>representation</i> of the data
The dependence of <i>representations</i> is a very common problem
Examples include:
</p>
<ul class="org-ul">
<li>Computer Science
<ul class="org-ul">
<li>Operations such as searching through data can be optimized through intelligent
indexing, as well as structure.</li>
</ul></li>
<li>Daily life
<ul class="org-ul">
<li>Arithmetic on Arabic numerals far easier for humans than Roman numerals</li>
</ul></li>
</ul>
<p>
Many AI tasks can be solved by extracting relevant <i>features</i> and then providing those to an ML alg
A useful <i>feature</i> for speaker identification from sound is an estimate of the size of the speaker's
vocal tract
</p>
<ul class="org-ul">
<li>Provides the ability to estimate how alike the speaker is to a man, woman or child.</li>
</ul>
<p>
However, many tasks are much more difficult to know what features to extract.
</p>
<ul class="org-ul">
<li>An example of this is writing a program to detect cars in photographs
<ul class="org-ul">
<li>We know cars have wheels, and would like to use the presence of wheels as a <i>feature</i>,
however, the difficulty comes from artifacts such as shadows, glare of metal parts,
objects hiding part of the wheel, etc.</li>
</ul></li>
</ul>
<p>
Logistic Regressions are an example of some of the limitations of ML in that they:
</p>
<ul class="org-ul">
<li>Learn how each of the <i>features</i> provided to it correlates with various known outcomes</li>
<li>Can't influence how <i>features</i> are defined in any way.
<ul class="org-ul">
<li>If an MRI scan of the patient were given, rather than the doctor's formalized report,
it couldn't take that data and turn it into something useful since the pixels in the MRI
scan don't correlate with potential complications</li>
</ul></li>
</ul>

<p>
The new concepts learned in this section are:
</p>
<dl class="org-dl">
<dt>Machine Learning</dt><dd>The capability of AI systems to acquire their own knowledge by extracting patterns
from raw data</dd>
<dt>Logistic Regression</dt><dd>Simple ML algorithm which can determine whether to recommend
<i>cesarean delivery</i> (C-Sections)</dd>
<dt>Naive Bayes</dt><dd>Simple ML algorithm which can separate legitimate e-mail from spam.</dd>
<dt>Feature</dt><dd>Each piece of information included in the <i>representation</i>
data they are given</dd>
</dl>
</div>
</div>
<div id="outline-container-orge609682" class="outline-3">
<h3 id="orge609682"><span class="section-number-3">1.5</span> Representation Learning</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>A solution to this problem is to use ML to discover <b>both</b>
<ul class="org-ul">
<li>The mapping from <i>representation</i> to output</li>
<li>The <i>representation</i> itself</li>
</ul></li>
<li>Learned <i>representations</i> often result in much better performance hand designed <i>representations</i></li>
<li>Enables AI to rapidly adapt to new tasks with minimal human intervention</li>
<li>Can discover a good set of <i>features</i> for simple tasks in minutes, and complex tasks in hours to months.
<ul class="org-ul">
<li>HUGE reduction in time, as manually designing <i>features</i> is a very complex task requiring a lot of time.</li>
<li>Compare to the potential of taking decades for an entire community of researchers.</li>
</ul></li>
<li>The typical example of representation learning is the <i>autoencoder</i></li>
</ul>
<p>
The new concepts learned in this section are:
</p>
<dl class="org-dl">
<dt>Encoder</dt><dd>Converts input data into a different <i>representation</i></dd>
<dt>Decoder</dt><dd>Converts the <i>encoded</i> representation back into the original format.</dd>
<dt>Autoencoder</dt><dd>the combination of the <i>encoder</i> function, and the <i>decoder</i> function.
<ul class="org-ul">
<li>Trained to preserve as much information as possible</li>
</ul></dd>
</dl>
</div>
</div>
<div id="outline-container-org9706d24" class="outline-3">
<h3 id="org9706d24"><span class="section-number-3">1.6</span> Design of Learning Features</h3>
<div class="outline-text-3" id="text-1-6">
<p>
When designing features or algorithms for learning features, our goal is usually to
separate the <i>factors of variation</i> that explain the observed data.
In this context, a factor is each separate source of incluence
</p>
<ul class="org-ul">
<li>Factors are often not directly observed, but rather unobserved forces or objects
in the physical world that affect observable quantities</li>
<li>Factors can be thought of as abstractions</li>
</ul>
<p>
Some examples of <i>factors of variation</i> for a speech recording are:
</p>
<ul class="org-ul">
<li>Speaker's age</li>
<li>Gender</li>
<li>Accent</li>
</ul>

<p>
<b>Most applications require us to <i>disentagle</i> the <i>factors of variation</i> and discard of the ones we
don't care about</b>
<i>Deep Learning</i> solves this problem by expressing representations in terms of other, simpler representations
This enables the computer to create complex concepts out of simpler ones.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: nerdsville</p>
<p class="date">Created: 2017-10-15 Sun 11:30</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
